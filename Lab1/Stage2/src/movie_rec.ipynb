{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    # latent_dim是离散特征隐向量的维度, feature_num是特征的数量\n",
    "    def __init__(self, feature_num, latent_dim):\n",
    "        super(FM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # 下面定义了三个矩阵\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([feature_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([feature_num, latent_dim]))\n",
    "\n",
    "    def forward(self, Input):\n",
    "        # 一阶交叉\n",
    "        order_1st = self.w0 + torch.mm(Input, self.w1)\n",
    "        # 二阶交叉\n",
    "        order_2nd = 1 / 2 * torch.sum(\n",
    "            torch.pow(torch.mm(Input, self.w2), 2) - torch.mm(torch.pow(Input, 2), torch.pow(self.w2, 2)), dim=1,\n",
    "            keepdim=True)\n",
    "        return order_1st + order_2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden, dropout=0):\n",
    "        super(DNN, self).__init__()\n",
    "        # 相邻的hidden层, Linear用于设置全连接层\n",
    "        # ModuleList可以将nn.Module的子类加入到List中\n",
    "        self.dnn = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden[:-1], hidden[1:]))])\n",
    "        # dropout用于训练, 代表前向传播中有多少概率神经元不被激活\n",
    "        # 为了减少过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn:\n",
    "            x = linear(x)\n",
    "            # relu激活函数\n",
    "            x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, hidden, feature_col, dropout=0):\n",
    "        super(DeepFM, self).__init__()\n",
    "        # 连续型特征和离散型特征\n",
    "        self.dense_col, self.sparse_col = feature_col\n",
    "        self.embedding_layer = nn.ModuleDict({\"embedding\" + str(i): nn.Embedding(num_embeddings=feature[\"feature_num\"],\n",
    "                                                                                 embedding_dim=feature[\"embedding_dim\"])\n",
    "                                              for i, feature in enumerate(self.sparse_col)})\n",
    "\n",
    "        self.feature_num = len(self.dense_col) + len(self.sparse_col) * self.sparse_col[0][\"embedding_dim\"]\n",
    "        # 将feature_num插入到hidden的开头\n",
    "        hidden.insert(0, self.feature_num)\n",
    "\n",
    "        self.fm = FM(self.feature_num, self.sparse_col[0][\"embedding_dim\"])\n",
    "        self.dnn = DNN(hidden, dropout)\n",
    "        # 最终输出, 将最后一层输入然后输出一维的结果\n",
    "        self.final = nn.Linear(hidden[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sparse_input, dense_input = x[:, :len(self.sparse_col)], x[:, len(self.sparse_col):]\n",
    "        sparse_input = sparse_input.long()\n",
    "        sparse_embed = [self.embedding_layer[\"embedding\" + str(i)](sparse_input[:, i]) for i in range(sparse_input.shape[1])]\n",
    "        # 按照最后一个维度拼接\n",
    "        sparse_embed = torch.cat(sparse_embed, dim=-1)\n",
    "\n",
    "        x = torch.cat([sparse_embed, dense_input], dim=-1)\n",
    "        wide_output = self.fm(x)\n",
    "        deep_output = self.final(self.dnn(x))\n",
    "        return F.sigmoid(torch.add(wide_output, deep_output)) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Movie  User  Time     favor   watched\n",
      "0        1181   383    27  0.065757  0.105539\n",
      "1         898   383    24  0.014995  0.009875\n",
      "2        1186   383    13  0.323543  0.386266\n",
      "3         804   383    10  0.321468  0.397589\n",
      "4        1169   383     9  0.206937  0.359602\n",
      "...       ...   ...   ...       ...       ...\n",
      "715019     40   371    11  0.000000  0.000000\n",
      "715020      2   371    11  0.000000  0.000000\n",
      "715021    852   371    11  0.000000  0.000000\n",
      "715022    711   371    11  0.000000  0.000000\n",
      "715023    678   371    10  0.000000  0.000000\n",
      "\n",
      "[715024 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "full_info = pd.read_csv('../Dataset/movie_final_info.csv')\n",
    "\n",
    "sparse_feature = [\"Movie\", \"User\", \"Time\"]\n",
    "dense_feature = [\"favor\", \"watched\"]\n",
    "# 填充缺失值\n",
    "full_info[sparse_feature] = full_info[sparse_feature].fillna('-1')\n",
    "full_info[dense_feature] = full_info[dense_feature].fillna(0)\n",
    "# 离散特征编码\n",
    "for feature in sparse_feature:\n",
    "    label = LabelEncoder()\n",
    "    full_info[feature] = label.fit_transform(full_info[feature])\n",
    "# 数值特征归一化\n",
    "mms = MinMaxScaler()\n",
    "full_info[dense_feature] = mms.fit_transform(full_info[dense_feature])\n",
    "print(full_info[sparse_feature + dense_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(full_info[sparse_feature + dense_feature].values.astype(np.float32), full_info[\"Rate\"].values.astype(np.float32),\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2023)\n",
    "# 构建数据管道\n",
    "train_dataset = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).float())\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).float(), torch.tensor(y_test).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 5.481974038055965, test_loss: 0.9939663989203317, ndcg_score: 0.9503540688848905\n",
      "epoch: 2, train_loss: 3.5784161124910625, test_loss: 0.861128158228738, ndcg_score: 0.9505360494468101\n",
      "epoch: 3, train_loss: 3.424962229388101, test_loss: 0.8491232071604048, ndcg_score: 0.9507985933566854\n",
      "epoch: 4, train_loss: 3.3989645430019926, test_loss: 0.8447950312069484, ndcg_score: 0.950722745104761\n",
      "epoch: 5, train_loss: 3.389598447935922, test_loss: 0.8431819404874529, ndcg_score: 0.9508207253202141\n",
      "epoch: 6, train_loss: 3.3827703424862454, test_loss: 0.8429012434823172, ndcg_score: 0.9507255888889335\n",
      "epoch: 7, train_loss: 3.3805575013160705, test_loss: 0.8444405674934388, ndcg_score: 0.9504600422285151\n",
      "epoch: 8, train_loss: 3.3792915463447573, test_loss: 0.842950907775334, ndcg_score: 0.9504324799403038\n",
      "epoch: 9, train_loss: 3.377736152921404, test_loss: 0.8410760709217616, ndcg_score: 0.9508659878919993\n",
      "epoch: 10, train_loss: 3.375027227401733, test_loss: 0.8418848548616682, ndcg_score: 0.9509511773752736\n"
     ]
    }
   ],
   "source": [
    "hidden = [128, 64, 32]\n",
    "dropout = 0\n",
    "feature_col = [[{\"feature\": feature_} for feature_ in dense_feature]] + [[{\"feature\": feature_, \"feature_num\": full_info[feature_].nunique(), \"embedding_dim\": 3} for feature_ in sparse_feature]]\n",
    "model = DeepFM(hidden, feature_col, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    loss_sum_train, loss_sum_test = 0.0, 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_prediction = model(x)\n",
    "        loss = criterion(y_prediction, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum_train += loss.item()\n",
    "        output_loss_train = loss_sum_train / (i + 1)\n",
    "\n",
    "    model.eval()\n",
    "    y_list, y_prediction_list = [], []\n",
    "    y_array, y_prediction_array = np.array([]), np.array([])\n",
    "    with torch.no_grad():\n",
    "        for index, (x_, y_) in enumerate(test_loader):\n",
    "            y_prediction_ = model(x_)\n",
    "            loss = criterion(y_prediction_, y_)\n",
    "            y_list = torch.tensor(y_).numpy()\n",
    "            y_array = np.append(y_array, y_list)\n",
    "            y_prediction_list = torch.tensor(y_prediction_).numpy()\n",
    "            y_prediction_array = np.append(y_prediction_array, y_prediction_)\n",
    "            loss_sum_test += loss.item()\n",
    "            output_loss_test = loss_sum_test / (i + 1)\n",
    "\n",
    "    y_array = y_array.tolist()\n",
    "    y_prediction_array = y_prediction_array.tolist()\n",
    "    ndcgscore = ndcg_score([y_array], [y_prediction_array])\n",
    "    print(\"epoch: {}, train_loss: {}, test_loss: {}, ndcg_score: {}\".format(epoch + 1, output_loss_train,\n",
    "                                                                            output_loss_test, ndcgscore))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
