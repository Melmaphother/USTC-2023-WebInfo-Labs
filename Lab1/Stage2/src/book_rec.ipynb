{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    # latent_dim是离散特征隐向量的维度, feature_num是特征的数量\n",
    "    def __init__(self, feature_num, latent_dim):\n",
    "        super(FM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # 下面定义了三个矩阵\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([feature_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([feature_num, latent_dim]))\n",
    "\n",
    "    def forward(self, Input):\n",
    "        # 一阶交叉\n",
    "        order_1st = self.w0 + torch.mm(Input, self.w1)\n",
    "        # 二阶交叉\n",
    "        order_2nd = 1 / 2 * torch.sum(\n",
    "            torch.pow(torch.mm(Input, self.w2), 2) - torch.mm(torch.pow(Input, 2), torch.pow(self.w2, 2)), dim=1,\n",
    "            keepdim=True)\n",
    "        return order_1st + order_2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden, dropout=0):\n",
    "        super(DNN, self).__init__()\n",
    "        # 相邻的hidden层, Linear用于设置全连接层\n",
    "        # ModuleList可以将nn.Module的子类加入到List中\n",
    "        self.dnn = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden[:-1], hidden[1:]))])\n",
    "        # dropout用于训练, 代表前向传播中有多少概率神经元不被激活\n",
    "        # 为了减少过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn:\n",
    "            x = linear(x)\n",
    "            # relu激活函数\n",
    "            x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, hidden, feature_col, dropout=0):\n",
    "        super(DeepFM, self).__init__()\n",
    "        # 连续型特征和离散型特征\n",
    "        self.dense_col, self.sparse_col = feature_col\n",
    "        self.embedding_layer = nn.ModuleDict({\"embedding\" + str(i): nn.Embedding(num_embeddings=feature[\"feature_num\"],\n",
    "                                                                                 embedding_dim=feature[\"embedding_dim\"])\n",
    "                                              for i, feature in enumerate(self.sparse_col)})\n",
    "\n",
    "        self.feature_num = len(self.dense_col) + len(self.sparse_col) * self.sparse_col[0][\"embedding_dim\"]\n",
    "        # 将feature_num插入到hidden的开头\n",
    "        hidden.insert(0, self.feature_num)\n",
    "\n",
    "        self.fm = FM(self.feature_num, self.sparse_col[0][\"embedding_dim\"])\n",
    "        self.dnn = DNN(hidden, dropout)\n",
    "        # 最终输出, 将最后一层输入然后输出一维的结果\n",
    "        self.final = nn.Linear(hidden[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sparse_input, dense_input = x[:, :len(self.sparse_col)], x[:, len(self.sparse_col):]\n",
    "        sparse_input = sparse_input.long()\n",
    "        sparse_embed = [self.embedding_layer[\"embedding\" + str(i)](sparse_input[:, i]) for i in range(sparse_input.shape[1])]\n",
    "        # 按照最后一个维度拼接\n",
    "        sparse_embed = torch.cat(sparse_embed, dim=-1)\n",
    "\n",
    "        x = torch.cat([sparse_embed, dense_input], dim=-1)\n",
    "        wide_output = self.fm(x)\n",
    "        deep_output = self.final(self.dnn(x))\n",
    "        return F.sigmoid(torch.add(wide_output, deep_output)) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Book  User  Time  raw_score  Be_Reading  Have_Read  Wanna_Read\n",
      "0        700  1653    28   0.948454    0.087177   0.037739    0.206488\n",
      "1        757  1653     1   0.845361    0.027265   0.010078    0.034117\n",
      "2        834  1653    30   0.855670    0.018542   0.008116    0.033353\n",
      "3        826  1653    25   0.855670    0.014965   0.019608    0.015190\n",
      "4       1181  1653    20   0.886598    0.302413   0.154366    0.361876\n",
      "...      ...   ...   ...        ...         ...        ...         ...\n",
      "637249   458  4150     3   0.876289    0.005703   0.022671    0.010555\n",
      "637250    14  4150     3   0.938144    0.034638   0.169296    0.100627\n",
      "637251     8  4150     3   0.907216    0.009684   0.014037    0.021897\n",
      "637252   136  4150     3   0.938144    0.006934   0.013486    0.006816\n",
      "637253   880  4150    28   0.814433    0.012063   0.042840    0.016488\n",
      "\n",
      "[637254 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "full_info = pd.read_csv('book_final_info.csv')\n",
    "\n",
    "sparse_feature = [\"Book\", \"User\", \"Time\"]\n",
    "dense_feature = [\"raw_score\", \"Be_Reading\", \"Have_Read\", \"Wanna_Read\"]\n",
    "# 填充缺失值\n",
    "full_info[sparse_feature] = full_info[sparse_feature].fillna('-1')\n",
    "full_info[dense_feature] = full_info[dense_feature].fillna(0)\n",
    "# 离散特征编码\n",
    "for feature in sparse_feature:\n",
    "    label = LabelEncoder()\n",
    "    full_info[feature] = label.fit_transform(full_info[feature])\n",
    "# 数值特征归一化\n",
    "mms = MinMaxScaler()\n",
    "full_info[dense_feature] = mms.fit_transform(full_info[dense_feature])\n",
    "print(full_info[sparse_feature + dense_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(full_info[sparse_feature + dense_feature].values.astype(np.float32), full_info[\"Rate\"].values.astype(np.float32),\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2023)\n",
    "# 构建数据管道\n",
    "train_dataset = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).float())\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).float(), torch.tensor(y_test).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 7.174833744049073, test_loss: 1.4051940269470216, ndcg_score: 0.9408075167613705\n",
      "epoch: 2, train_loss: 4.59050658416748, test_loss: 1.1189309616088867, ndcg_score: 0.9404307256168711\n",
      "epoch: 3, train_loss: 4.308561996459961, test_loss: 1.0945753288269042, ndcg_score: 0.9403987023577233\n",
      "epoch: 4, train_loss: 4.260613311767578, test_loss: 1.0880211143493652, ndcg_score: 0.9404357605895934\n",
      "epoch: 5, train_loss: 4.245028644561768, test_loss: 1.0880177268981934, ndcg_score: 0.9400013421923703\n",
      "epoch: 6, train_loss: 4.239394046783447, test_loss: 1.08648876953125, ndcg_score: 0.9406631805996507\n",
      "epoch: 7, train_loss: 4.238051387786865, test_loss: 1.087811222076416, ndcg_score: 0.9398094358556313\n",
      "epoch: 8, train_loss: 4.233310882568359, test_loss: 1.0831170616149903, ndcg_score: 0.9404004727488062\n",
      "epoch: 9, train_loss: 4.233950477600097, test_loss: 1.083015567779541, ndcg_score: 0.9401586607093397\n",
      "epoch: 10, train_loss: 4.225923812866211, test_loss: 1.0829881477355956, ndcg_score: 0.9405959723507594\n"
     ]
    }
   ],
   "source": [
    "hidden = [128, 64, 32]\n",
    "dropout = 0\n",
    "feature_col = [[{\"feature\": feature_} for feature_ in dense_feature]] + [[{\"feature\": feature_, \"feature_num\": full_info[feature_].nunique(), \"embedding_dim\": 3} for feature_ in sparse_feature]]\n",
    "model = DeepFM(hidden, feature_col, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    loss_sum_train, loss_sum_test = 0.0, 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_prediction = model(x)\n",
    "        loss = criterion(y_prediction, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum_train += loss.item()\n",
    "        output_loss_train = loss_sum_train / (i + 1)\n",
    "\n",
    "    model.eval()\n",
    "    y_list, y_prediction_list = [], []\n",
    "    y_array, y_prediction_array = np.array([]), np.array([])\n",
    "    with torch.no_grad():\n",
    "        for index, (x_, y_) in enumerate(test_loader):\n",
    "            y_prediction_ = model(x_)\n",
    "            loss = criterion(y_prediction_, y_)\n",
    "            y_list = torch.tensor(y_).numpy()\n",
    "            y_array = np.append(y_array, y_list)\n",
    "            y_prediction_list = torch.tensor(y_prediction_).numpy()\n",
    "            y_prediction_array = np.append(y_prediction_array, y_prediction_)\n",
    "            loss_sum_test += loss.item()\n",
    "            output_loss_test = loss_sum_test / (i + 1)\n",
    "\n",
    "    y_array = y_array.tolist()\n",
    "    y_prediction_array = y_prediction_array.tolist()\n",
    "    ndcgscore = ndcg_score([y_array], [y_prediction_array])\n",
    "    print(\"epoch: {}, train_loss: {}, test_loss: {}, ndcg_score: {}\".format(epoch + 1, output_loss_train,\n",
    "                                                                            output_loss_test, ndcgscore))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
